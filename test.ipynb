{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/hdd1/user19/bag/2.fastText/src\")\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import Dictionary, CustumDataset\n",
    "from model import SISG\n",
    "from utils import *\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = f\"../src/{lang}_dictionary.pkl\"\n",
    "with open(file_path,\"rb\") as file:\n",
    "    dictionaries = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2668582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionaries.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"input your path\"\n",
    "device = \"cpu\"\n",
    "model_info = torch.load(model_path, map_location=device)\n",
    "best_model = model_info['model']\n",
    "best_model.load_state_dict(model_info['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, base_corr = load_zg222()\n",
    "vocab_size = len(dictionaries.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = [3,4,5,6]\n",
    "oov = 0\n",
    "corr_list = []\n",
    "b_corr_list = []\n",
    "with torch.no_grad():\n",
    "    for word1, word2, bc in zip(w1, w2, base_corr):\n",
    "        word1, word2 = word1.lower(), word2.lower()\n",
    "        if word1 in dictionaries.word2id.keys():\n",
    "            word1_id = torch.LongTensor([dictionaries.word2id[word1]]).to(device)\n",
    "            word1_subword = list(map(lambda x: (x+vocab_size)%MAX_SUBWORD_VOCAB_SIZE, dictionaries.word2subword[word1_id.item()]))\n",
    "            word1_id %= MAX_SUBWORD_VOCAB_SIZE\n",
    "            if len(word1_subword) == 1:\n",
    "                word1_e = best_model.word_in_emb(word1_id).view(-1).to(device=device)\n",
    "                l2norm = word1_e.pow(2).sum().sqrt().to(device=device)\n",
    "                word1_emb = word1_e/(l2norm)\n",
    "            else:\n",
    "                word1_subword = torch.LongTensor(word1_subword).to(device)\n",
    "                word1_subword_emb = best_model.word_in_emb(word1_subword).mean(dim=0)\n",
    "                word1_emb = best_model.word_in_emb(word1_id).view(-1)\n",
    "                word1_emb += word1_subword_emb\n",
    "                l2_norm = word1_emb.pow(2).sum().sqrt()\n",
    "                word1_emb /= (l2_norm)\n",
    "        else:\n",
    "            word1_emb = torch.zeros(300).to(device)\n",
    "            oov += 1\n",
    "            # continue\n",
    "            word1_subword = get_subword(word1, ngram, dictionaries)\n",
    "            word1_subword = list(map(lambda x: (x+vocab_size)%MAX_SUBWORD_VOCAB_SIZE, word1_subword))\n",
    "            word1_subword = torch.LongTensor(word1_subword).to(device)\n",
    "            word1_emb = best_model.word_in_emb(word1_subword).mean(dim=0)\n",
    "            l2_norm = word1_emb.pow(2).sum().sqrt()\n",
    "            word1_emb = word1_emb/(l2_norm + 1e-7)\n",
    "            \n",
    "        if word2 in dictionaries.word2id.keys():\n",
    "            word2_id = torch.LongTensor([dictionaries.word2id[word2]]).to(device)\n",
    "            word2_subword = list(map(lambda x: (x+vocab_size)%MAX_SUBWORD_VOCAB_SIZE, dictionaries.word2subword[word2_id.item()]))\n",
    "            word2_id %= MAX_SUBWORD_VOCAB_SIZE\n",
    "            if len(word2_subword) == 1:\n",
    "                word2_e = best_model.word_in_emb(word2_id).view(-1).to(device=device)\n",
    "                l2norm = word2_e.pow(2).sum().sqrt().to(device=device)\n",
    "                word2_emb = word2_e/(l2norm)\n",
    "            else:\n",
    "                word2_subword = torch.LongTensor(word2_subword).to(device)\n",
    "                word2_subword_emb = best_model.word_in_emb(word2_subword).mean(dim=0)\n",
    "                word2_emb = best_model.word_in_emb(word2_id).view(-1)\n",
    "                word2_emb += word2_subword_emb\n",
    "                l2_norm = word2_emb.pow(2).sum().sqrt()\n",
    "                word2_emb /= (l2_norm)\n",
    "        else:\n",
    "            word2_emb = torch.zeros(300).to(device)\n",
    "            oov += 1\n",
    "            # continue\n",
    "            word2_subword = get_subword(word2, ngram, dictionaries)\n",
    "            word2_subword = list(map(lambda x: (x+vocab_size)%MAX_SUBWORD_VOCAB_SIZE, word2_subword))\n",
    "            word2_subword = torch.LongTensor(word2_subword).to(device)\n",
    "            word2_emb = best_model.word_in_emb(word2_subword).mean(dim=0)\n",
    "            l2_norm = word2_emb.pow(2).sum().sqrt()\n",
    "            word2_emb = word2_emb/(l2_norm + 1e-7)\n",
    "        corr_list.append(torch.sum(word1_emb* word2_emb).item())\n",
    "        b_corr_list.append(bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_corr_list = np.array(list(map(float,b_corr_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm = np.sqrt(np.sum(np.power(b_corr_list,2)))\n",
    "b_corr_list /= (l2_norm + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047533804937815824\n"
     ]
    }
   ],
   "source": [
    "corr = stats.spearmanr(corr_list, b_corr_list)[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_num = len(dictionaries.ids)\n",
    "\n",
    "word_vec = torch.zeros((len(dictionaries.ids),300)).to(device=device)\n",
    "with torch.no_grad():\n",
    "    best_model.eval()\n",
    "    for iii in tqdm(dictionaries.ids):\n",
    "        subword_id = list(map(lambda x: (x+vocab_num)%MAX_SUBWORD_VOCAB_SIZE, dictionaries.word2subword[iii]))\n",
    "        # subword_id = subword_id\n",
    "        if len(subword_id) == 0:\n",
    "            word_e = best_model.word_in_emb(torch.LongTensor([iii])).view(-1).to(device=device)\n",
    "            l2norm = word_e.pow(2).sum().sqrt().to(device=device)\n",
    "            word_vec[iii] = word_e/l2norm\n",
    "            continue\n",
    "        subword_id = torch.LongTensor(subword_id).to(device=device)\n",
    "        # subword_id = torch.LongTensor(subword_id + [iii]).to(device=device)\n",
    "        si = best_model.word_in_emb(subword_id).to(device=device)\n",
    "        si = si.mean(dim=0).to(device=device)\n",
    "        si += best_model.word_in_emb(torch.LongTensor([iii])).view(-1).to(device=device)\n",
    "        l2norm = si.pow(2).sum().sqrt().to(device=device) #L2Norm\n",
    "        word_vec[iii] = si/l2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_words(path = \"../questions-words.txt\"):\n",
    "    with open(path, 'r', encoding = \"UTF8\") as f:\n",
    "        temp = f.readlines()\n",
    "\n",
    "    semantic_words = []\n",
    "    syntatic_words = []\n",
    "    for e in temp:\n",
    "        t = e[:-1].split(\" \")\n",
    "        if t[1] == \"gram2-opposite\":\n",
    "        # if t[1] == \"gram1-adjective-to-adverb\":\n",
    "            break\n",
    "        if t[0] == \":\":\n",
    "            continue\n",
    "        words = [tt.lower() for tt in t]\n",
    "        semantic_words.append(words)\n",
    "    for e in temp[::-1]:\n",
    "        t = e[:-1].split(\" \")\n",
    "        if t[1] == \"gram2-opposite\":\n",
    "        # if t[1] == \"gram1-adjective-to-adverb\":\n",
    "            break\n",
    "        if t[0] == \":\":\n",
    "            continue\n",
    "        words = [tt.lower() for tt in t]\n",
    "        syntatic_words.append(words)\n",
    "\n",
    "    return np.array(semantic_words), np.array(syntatic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem, syn = test_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_size = 0\n",
    "sem_score = 0\n",
    "for anal_words in tqdm(sem):\n",
    "    if anal_words[0] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[1] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[2] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[3] not in dictionaries.word2id:\n",
    "        continue\n",
    "    sem_size += 1\n",
    "    \n",
    "    word1, word2, word3, word4 = list(map(lambda x:dictionaries.word2id[x], anal_words))\n",
    "    query = word_vec[word2] - word_vec[word1] + word_vec[word3]\n",
    "    similarities = torch.matmul(query, word_vec.T)\n",
    "    similarities[[word1, word2, word3]] = 0\n",
    "    predict = torch.topk(similarities, k=1)[1]\n",
    "    if word4 in predict:\n",
    "        sem_score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_size = 0\n",
    "syn_score = 0\n",
    "for anal_words in tqdm(syn):\n",
    "    if anal_words[0] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[1] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[2] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[3] not in dictionaries.word2id:\n",
    "        continue\n",
    "    syn_size += 1\n",
    "    \n",
    "    word1, word2, word3, word4 = list(map(lambda x:dictionaries.word2id[x], anal_words))\n",
    "    query = word_vec[word2] - word_vec[word1] + word_vec[word3]\n",
    "    similarities = torch.matmul(query, word_vec.T)\n",
    "    similarities[[word1, word2, word3]] = 0\n",
    "    predict = torch.topk(similarities, k=1)[1]\n",
    "    if word4 in predict:\n",
    "        syn_score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_score/sem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_score/syn_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"de\"\n",
    "file_path = f\"../{lang}_dictionary_v0.pkl\"\n",
    "with open(file_path,\"rb\") as file:\n",
    "    dictionaries = pickle.load(file)\n",
    "model_path = f\"../sisg_{lang}_shuffle_v0.pth\"\n",
    "device = \"cpu\"\n",
    "model_info = torch.load(model_path, map_location=device)\n",
    "best_model = model_info['model']\n",
    "best_model.load_state_dict(model_info['model_state_dict'])\n",
    "\n",
    "vocab_num = len(dictionaries.ids)\n",
    "\n",
    "word_vec = torch.zeros((len(dictionaries.ids),300)).to(device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    best_model.eval()\n",
    "    for iii in tqdm(dictionaries.ids):\n",
    "        subword_id = list(map(lambda x: (x+vocab_num)%MAX_SUBWORD_VOCAB_SIZE, dictionaries.word2subword[iii]))\n",
    "        # subword_id = subword_id\n",
    "        if len(subword_id) == 0:\n",
    "            word_e = best_model.word_in_emb(torch.LongTensor([iii])).view(-1).to(device=device)\n",
    "            l2norm = word_e.pow(2).sum().sqrt().to(device=device)\n",
    "            word_vec[iii] = word_e/l2norm\n",
    "            continue\n",
    "        subword_id = torch.LongTensor(subword_id).to(device=device)\n",
    "        # subword_id = torch.LongTensor(subword_id + [iii]).to(device=device)\n",
    "        si = best_model.word_in_emb(subword_id).to(device=device)\n",
    "        si = si.mean(dim=0).to(device=device)\n",
    "        si += best_model.word_in_emb(torch.LongTensor([iii])).view(-1).to(device=device)\n",
    "        l2norm = si.pow(2).sum().sqrt().to(device=device) #L2Norm\n",
    "        word_vec[iii] = si/l2norm\n",
    "        \n",
    "sem, syn = test_words(path = \"../de_trans_Google_analogies.txt\")\n",
    "\n",
    "sem_size = 0\n",
    "sem_score = 0\n",
    "for anal_words in tqdm(sem):\n",
    "    if anal_words[0] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[1] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[2] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[3] not in dictionaries.word2id:\n",
    "        continue\n",
    "    sem_size += 1\n",
    "    \n",
    "    word1, word2, word3, word4 = list(map(lambda x:dictionaries.word2id[x], anal_words))\n",
    "    query = word_vec[word2] - word_vec[word1] + word_vec[word3]\n",
    "    similarities = torch.matmul(query, word_vec.T)\n",
    "    similarities[[word1, word2, word3]] = 0\n",
    "    predict = torch.topk(similarities, k=1)[1]\n",
    "    if word4 in predict:\n",
    "        sem_score += 1\n",
    "        \n",
    "syn_size = 0\n",
    "syn_score = 0\n",
    "for anal_words in tqdm(syn):\n",
    "    if anal_words[0] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[1] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[2] not in dictionaries.word2id:\n",
    "        continue\n",
    "    if anal_words[3] not in dictionaries.word2id:\n",
    "        continue\n",
    "    syn_size += 1\n",
    "    \n",
    "    word1, word2, word3, word4 = list(map(lambda x:dictionaries.word2id[x], anal_words))\n",
    "    query = word_vec[word2] - word_vec[word1] + word_vec[word3]\n",
    "    similarities = torch.matmul(query, word_vec.T)\n",
    "    similarities[[word1, word2, word3]] = 0\n",
    "    predict = torch.topk(similarities, k=1)[1]\n",
    "    if word4 in predict:\n",
    "        syn_score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_score/sem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_score/syn_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
